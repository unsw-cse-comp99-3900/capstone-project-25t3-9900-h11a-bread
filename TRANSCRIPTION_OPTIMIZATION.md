# 语音转录优化说明

## 🎯 优化目标
解决语音转录的三个主要问题：
1. 延迟高，跟不上正常语速
2. 经常丢失单词
3. 单词识别错误率高

## 🔧 优化措施

### 1. 增加音频帧大小 (最重要)
**修改前：** 480 samples (30ms)  
**修改后：** 1920 samples (120ms)

**原因：**
- 30ms的帧太小，导致音频过度碎片化
- Speechmatics API 需要足够的音频上下文才能准确识别
- 120ms帧是实时转录的最佳平衡（延迟可接受，准确度高）

**影响：**
- ✅ 减少网络传输次数（从每秒33帧降到每秒8帧）
- ✅ 给 ASR 模型更多上下文信息
- ✅ 减少音频碎片导致的识别错误

### 2. 降低 max_delay 延迟
**修改前：** 2.0 秒  
**修改后：** 1.0 秒

**原因：**
- 2秒延迟对实时对话来说太慢
- Speechmatics 最小要求是 0.7秒，1.0秒是好的平衡

**影响：**
- ✅ 更快的响应速度
- ✅ 更接近实时的体验

### 3. 启用浏览器音频处理
**修改前：** 
```javascript
echoCancellation: false,
noiseSuppression: false,
autoGainControl: false
```

**修改后：**
```javascript
echoCancellation: true,  // 回声消除
noiseSuppression: true,  // 降噪
autoGainControl: true    // 自动增益
```

**原因：**
- 浏览器的音频处理算法很成熟
- 可以有效去除背景噪音和回声
- 提供更稳定的音量水平

**影响：**
- ✅ 更清晰的音频输入
- ✅ 更稳定的识别效果
- ✅ 适应不同的麦克风质量

### 4. 降低后端降噪强度
**修改前：** subtract_scale = 1.0  
**修改后：** subtract_scale = 0.8

**原因：**
- 过度降噪会导致音频失真
- 与浏览器降噪配合，避免双重处理

**影响：**
- ✅ 保留更多原始语音特征
- ✅ 减少过度处理导致的失真

### 5. 禁用说话人分离（单人场景）
**修改前：** diarization = "speaker"  
**修改后：** diarization = None

**原因：**
- 说话人分离增加处理复杂度
- 单人说话场景下不需要
- 可能导致不必要的分割和错误

**影响：**
- ✅ 提高单人说话的识别准确度
- ✅ 减少不必要的文本分割
- ✅ 降低处理延迟

### 6. 明确指定语言
**修改：** 保持 language = "en"（不使用自动检测）

**原因：**
- 自动语言检测需要时间和更多音频
- 可能误判导致错误的模型

**影响：**
- ✅ 更快的启动速度
- ✅ 更准确的识别（使用正确的语言模型）

## 📊 预期效果对比

| 指标 | 优化前 | 优化后 | 改善 |
|------|--------|--------|------|
| 音频帧大小 | 30ms | 120ms | ✅ 4倍 |
| 每秒传输次数 | 33次 | 8次 | ✅ 75%减少 |
| 转录延迟 | ~2秒 | ~1秒 | ✅ 50%降低 |
| 网络开销 | 高 | 低 | ✅ 显著降低 |
| 识别准确度 | 低 | 高 | ✅ 显著提升 |

## 🚀 如何测试

1. **重启后端服务：**
   ```bash
   cd backend
   # 如果用 Docker
   docker-compose down && docker-compose up backend-dev
   
   # 如果本地运行
   # Ctrl+C 停止，然后重新运行
   uvicorn VAD.app:app --reload --host 0.0.0.0 --port 8000
   ```

2. **重启前端服务：**
   - 在运行 `npm run dev` 的终端按 `Ctrl+C` 停止
   - 重新运行 `npm run dev`
   - 刷新浏览器

3. **测试建议：**
   - 用正常语速说完整的句子
   - 测试不同的音量和距离
   - 尝试有背景噪音的环境
   - 观察实时反馈和最终结果

## 📝 进一步优化建议

如果还有问题，可以尝试：

1. **调整帧大小：**
   - 增加到 2400 samples (150ms) - 更高准确度，稍高延迟
   - 减少到 1440 samples (90ms) - 稍低延迟，可能降低准确度

2. **调整 max_delay：**
   - 降低到 0.8秒 - 更快响应，可能影响准确度
   - 增加到 1.5秒 - 更高准确度，稍高延迟

3. **检查网络质量：**
   - 使用本地运行而不是 Docker（减少网络跳跃）
   - 检查 WebSocket 连接的稳定性

4. **音频质量：**
   - 使用质量更好的麦克风
   - 在安静的环境中测试
   - 说话清晰，避免太快或太慢

## ⚠️ 注意事项

- 如果需要多人说话场景，设置 `diarization: "speaker"`
- 如果识别非英语，修改 `language` 参数（如 "zh", "es" 等）
- frame_samples 必须能被音频缓冲区大小整除
- 修改后需要重启前后端服务才能生效

## 🎉 总结

这次优化主要通过：
1. **增加帧大小** - 给 AI 模型更多上下文
2. **优化音频质量** - 启用浏览器处理
3. **减少延迟** - 降低 max_delay
4. **简化处理** - 移除不必要的功能

应该能显著改善转录质量和响应速度！

